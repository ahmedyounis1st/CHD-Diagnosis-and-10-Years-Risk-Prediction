# -*- coding: utf-8 -*-
"""final_project_chd_diagnostic_NN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hsTi9VGz8AwDTmyW5eW336K64WhuGqGi
"""

#import Libraries 
from tensorflow import keras
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler , StandardScaler
from sklearn.model_selection import train_test_split 
import matplotlib.pyplot as plt
import seaborn as sns

#import data
chd_data=pd.read_csv('/content/CHDdata diagnosis.csv')
chd_data

#check for null value 
chd_data.isnull().sum()

#check for duplicated
chd_data.duplicated().sum()

#check data types
chd_data.dtypes

from sklearn import preprocessing
le = preprocessing.LabelEncoder()
le.fit(chd_data.iloc[:,4])
chd_data.iloc[:,4]=le.transform(chd_data.iloc[:,4])

chd_data

Index = np.r_[0:4,5:9]
plt.figure(figsize=(16,5))
chd_data.iloc[:,Index].boxplot()
plt.title("Distribution of the values ​​of all potential predictors")
plt.show()

outliers = ['sbp', 'tobacco', 'ldl', 'typea', 'obesity', 'alcohol']
for column in outliers:
  Q1,Q3 = np.percentile(chd_data[column],[25,75])
  IQR = Q3 - Q1
  lower_fence = Q1 - (1.5*IQR)
  upper_fence = Q3 + (1.5*IQR)  
  chd_data[column] = chd_data[column].apply(lambda x: upper_fence if x>upper_fence
                                              else lower_fence if x<lower_fence else x)

plt.figure(figsize=(16,5))
sns.boxplot(data=chd_data.iloc[:,Index])
plt.title("Distribution of the values ​​of all potential predictors")
plt.grid()
plt.show()

chd_data.to_csv('diagnosis_clean.csv', index=False)

df_clean = pd.read_csv('diagnosis_clean.csv')
df_clean.head()

#rescaling
scaler = StandardScaler()
chd_data.iloc[:,Index] = scaler.fit_transform(chd_data.iloc[:,Index])
chd_data.iloc[0:5,:]

plt.figure(figsize=(16,5))
sns.boxplot(data=chd_data.iloc[:,Index])
plt.title("Distribution of the values ​​of all potential standardized predictors")
plt.grid()
plt.show()

#spliting data
x=chd_data.iloc[:,:-1]
y=chd_data.iloc[:,-1]
x

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

x_train.iloc[0,:]

model = keras.Sequential()

model.add(keras.layers.Dense(2, input_shape= (9,), activation = 'softmax'))

from tensorflow.keras import optimizers

RMS = optimizers.RMSprop(
    learning_rate=0.001,
    rho=0.9,
    momentum=0.0,
    epsilon=1e-07,
    centered=False,
    name="RMSprop"
    )

adam = optimizers.Adam(
    learning_rate=0.002, 
    beta_1=0.9, 
    beta_2=0.999, 
    epsilon=1e-07, 
    decay=0.0,
    name="adam"
    )

lr_schedule = optimizers.schedules.ExponentialDecay(
    initial_learning_rate=1e-2,
    decay_steps=10000,
    decay_rate=0.9)
sgd = optimizers.SGD(learning_rate=lr_schedule, name ='sgd')

# model.compile(optimizer= 'adagrad', loss='binary_crossentropy', metrics=['binary_accuracy'])
model.compile(optimizer= 'sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

callback1 = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights= True)
callback2 = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights= True)
results = model.fit(
    x = x_train,
    y = y_train,
    shuffle=True,
    epochs = 100,
    batch_size = 32,
    # callbacks=[callback1, callback1],
    validation_data = (x_test, y_test)
)

eval = model.evaluate(x = x_test, y = y_test)

import matplotlib.pyplot as plt
plt.plot(results.history['loss'])
plt.plot(results.history['val_loss'])
plt.legend(['Training', 'Validation'])
plt.title('Training and Validation Losses')
plt.xlabel('epoch')
plt.ylabel('Losses')

plt.plot(results.history['accuracy'])
plt.plot(results.history['val_accuracy'])
plt.legend(['Training', 'Validation'])
plt.title('Training and Validation accuarcy')
plt.xlabel('epoch')
plt.ylabel('accuarcy')

model.summary()

model.save('final_project_chd_diagnostic_NN.h5')

((model.predict(x_train.iloc[0:25,:]) >= 0.5).astype(int)).reshape(1,-1)

model.predict(x_train.iloc[0:1,:])